\documentclass[11pt, final]{article}

% This file will be kept up-to-date at the following GitHub repository:
%
% https://github.com/automl-conf/LatexTemplate
%
% Please file any issues/bug reports, etc. you may have at:
%
% https://github.com/automl-conf/LatexTemplate/issues

\usepackage{microtype} % microtypography
\usepackage{booktabs}  % tables
\usepackage{url}  % url


% AMS math
\usepackage{amsmath}
\usepackage{amsthm}

% With no package options, the submission will be anonymized, the supplemental
% material will be suppressed, and line numbers will be added to the manuscript.
%
% To hide the supplementary material (e.g., for the first submission deadline),
% use the [hidesupplement] option:
%
% \usepackage[hidesupplement]{automl}
%
% To compile a non-anonymized camera-ready version, add the [final] option:
%
% \usepackage[final]{automl}
%
% or
%
% \usepackage[final, hidesupplement]{automl}

\usepackage[hidesupplement]{automl}

% You may use any reference style as long as you are consistent throughout the
% document. As a default we suggest author--year citations; for bibtex and
% natbib you may use:

\usepackage{natbib}
\bibliographystyle{apalike}

% and for biber and biblatex you may use:

% \usepackage[%
%   backend=biber,
%   style=authoryear-comp,
%   sortcites=true,
%   natbib=true,
%   giveninits=true,
%   maxcitenames=2,
%   doi=false,
%   url=true,
%   isbn=false,
%   dashed=false
% ]{biblatex}
% \addbibresource{...}

\title{Example: A Classical Algorithm Configuration Approach to Dynamic Learning Rate Configuration}

% The syntax for adding an author is
%
% \author[i]{\nameemail{author name}{author email}}
%
% where i is an affiliation counter. Authors may have
% multiple affiliations; e.g.:
%
% \author[1,2]{\nameemail{Anonymous}{anonymous@example.com}}

\author[1]{\nameemail{Steven Adriaensen}{adriaens@cs.uni-freiburg.de}}
\author[1]{\nameemail{G\"{o}ktu\u{g} Karaka\c{s}l{\i}}{karakasg@cs.uni-freiburg.de}}

% the list might continue:
% \author[2,3]{\nameemail{Author 2}{email2@example.com}}
% \author[3]{\nameemail{Author 3}{email3@example.com}}
% \author[4]{\nameemail{Author 4}{email4@example.com}}

% if you need to force a linebreak in the author list, prepend an \author entry
% with \\:

% \author[3]{\\\nameemail{Author 5}{email5@example.com}}

% Specify corresponding affiliations after authors, referring to counter used in
% \author:

\affil[1]{University of Freiburg}

% the list might continue:
% \affil[2]{Institution 2}
% \affil[3]{Institution 3}
% \affil[4]{Institution 4}

% define PDF metadata, please fill in to aid in accessibility of the resulting PDF
\hypersetup{%
  pdfauthor={}, % will be reset to "Anonymous" unless the "final" package option is given
  pdftitle={},
  pdfsubject={},
  pdfkeywords={}
}

\begin{document}

\maketitle

\noindent Here we solve DAC~\cite{biedenkapp-ecai20} by reduction to a classical (static) algorithm configuration problem. Specifically, we statically tune the parameters of hand-crafted dynamic learning rate schedulers using SMAC~\cite{hutter-lion11a}. We illustrate this approach for four different schedulers (e.g., see \texttt{schedulers.py}):
\begin{description}
\item[\texttt{ConstantLRPolicy}:] A scheduler using the same learning rate (configurable parameter) throughout the training process.
\item[\texttt{CosineAnnealingLRPolicy}:] A cosine annealing scheduler \cite{loshchilov-iclr17a}. Starting from an initial learning rate (configurable parameter) reduces the learning rate to 0 following a re-scaled half cosine curve.
\item[\texttt{SimpleReactivePolicy}:] A novel simple scheduler using a constant learning rate (configurable parameter) for the first 2 epochs. After every $(2+i)^{\textrm{th}}$ epoch ($i \in \mathbb{N}$), if the average mini-batch loss during the last epoch was lower than that in the previous 2 epochs, it \emph{increases} the learning rate with a factor $a$ (configurable parameter). If the loss is higher, it \emph{decreases} the learning rate with a factor ${b}$ (configurable parameter). 
\item[\texttt{ReduceLROnPlateauPolicy}:] A scheduler emulating the \href{https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html}{ReduceLROnPlateau} pytorch scheduler. It adjusts the learning rate per epoch, based on the validation loss (observed at the end of every epoch). In particular, it reduces the learning rate with a `factor' if the validation loss has stagnated for `patience' epochs. This scheduler has many parameters, but we only optimized the initial learning rate, `patience', `factor', and used the pytorch defaults for the others.
\end{description}
We provide the code to configure these schedulers using SMAC (see \texttt{train.py}) as well as the resulting optimized configurations (see \texttt{\url{tmp/saved_configs}}) on 1000 training instances (environment seed was 42) after 5000 target algorithm evaluations (smac seed was 42). Note that the training code requires installing \href{https://github.com/automl/SMAC3}{SMAC3} (e.g., calling \texttt{pip install -r requirements\_train.txt}).

\bibliography{references}
\end{document}
